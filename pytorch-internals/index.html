<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>Pytorch Internals: Tensors and Autograd · termina's Blog</title><meta name="description" content="For all we concern, Pytorch &amp;#x3D; tensor + autograd
Wrappers on Wrappers on WrappersFrom low-level to high-level:
TensorStorage and StridesStrides is"><meta name="og:description" content="For all we concern, Pytorch &amp;#x3D; tensor + autograd
Wrappers on Wrappers on WrappersFrom low-level to high-level:
TensorStorage and StridesStrides is"><meta name="twitter:site" content="termina's Blog"><meta name="twitter:title" content="Pytorch Internals: Tensors and Autograd"><meta name="twitter:card" content="summary"><meta name="keywords" content="termina, terd, terminal dogma, blog, 博客, 计算机, 计算机科学, CS, compsci, computer science, Mac, macos, photography, fujifilm, 摄影, book reviews, 书评, 散文, 杂感, emo"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 6.3.0"><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="container" id="stage"><div class="row"><div class="col-sm-3 col-xs-12 side-container invisible" id="side-bar"><div class="vertical-text site-title"><h3 class="site-title-small" tabindex="-1"><a class="a-title" href="/">百無一用，昆亂不擋。</a></h3><h1 class="site-title-large" tabindex="-1"><a class="a-title" href="/">termina's Blog</a></h1></div><br class="visible-lg visible-md visible-sm"><div class="site-title-links" id="site-nav"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li><a href="/about/index.html">关于我</a></li><li class="soc"><a href="https://github.com/TerminaD" target="_blank" rel="noopener noreferrer" aria-label="Github"><i class="fa fa-github">&nbsp;</i></a><a href="https://twitter.com/BenY88347017" target="_blank" rel="noopener noreferrer" aria-label="Twitter"><i class="fa fa-twitter">&nbsp;</i></a><a href="https://termina.me/atom.xml" target="_blank" rel="noopener noreferrer" aria-label="RSS"><i class="fa fa-rss">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2023&nbsp;<a target="_blank" href="https://termina.me" rel="noopener noreferrer">termina</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div class="col-sm-9 col-xs-12 main-container invisible" id="main-container"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>Pytorch Internals: Tensors and Autograd</a></p><p class="post-meta"><span class="date meta-item">发布于&nbsp;2023-07-14</span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a class="a-tag" href="/tags/计算机科学/" title="计算机科学">计算机科学</a><span>&nbsp;</span><a class="a-tag" href="/tags/人工智能/" title="人工智能">人工智能</a><span>&nbsp;</span></span></p><p class="post-abstract"></p><p>For all we concern, <strong>Pytorch &#x3D; tensor + autograd</strong></p><h1 id="Wrappers-on-Wrappers-on-Wrappers"><a href="#Wrappers-on-Wrappers-on-Wrappers" class="headerlink" title="Wrappers on Wrappers on Wrappers"></a>Wrappers on Wrappers on Wrappers</h1><p>From low-level to high-level:<br><img src="/pytorch-internals/1.png"></p><h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><h2 id="Storage-and-Strides"><a href="#Storage-and-Strides" class="headerlink" title="Storage and Strides"></a>Storage and Strides</h2><p>Strides is a tuple that defines how many elements you need to traverse in memory before you reach the next row&#x2F;column in a dimension in the tensor, i.e. the dimension of the tensor</p><p>When transposing or resizing a matrix, only the strides are modified, not the storage.</p><p>This leads to the concept of <strong>contiguous</strong>: a tensor is contiguous if and only if its storage is laid out from the outermost dimension to the inner most, i.e. a normal memory layout</p><p><strong>Why does contiguousness matter?</strong> Some operations, such as <code>view</code>, only works for contiguous tensors.</p><p><code>view</code> vs. <code>reshape</code>:</p><ol><li>When contiguous, they both return a different tensor of the same original storage</li><li>However, when non-continuous, <code>view</code> returns an error while <code>reshape</code> creates a contiguous copy of the input tensor’s storage and return the copied storage’s tensor</li></ol><h2 id="Dispatching-Tensor-Operations"><a href="#Dispatching-Tensor-Operations" class="headerlink" title="Dispatching Tensor Operations"></a>Dispatching Tensor Operations</h2><ol><li>Dynamically (Run-time) dispatched according to device</li><li>Dispatched according to dtype</li></ol><h2 id="Extending-Tensors"><a href="#Extending-Tensors" class="headerlink" title="Extending Tensors"></a>Extending Tensors</h2><p><strong>Method 1: The Extension Point Trinity</strong><br>Device (CPU, cuda, xla, hip, fpga…)<br>×<br>Layout (strided, sparse, mkldnn…)<br>×<br>dtype (float, double, int, long, bool…)</p><p>Write respective kernels for each of these combinations</p><p><strong>Method 2: A Python Wrapper Class around Tensor</strong></p><p>Which to use? If the tensor needs to be passed in autograd, make it a Pytorch extension</p><h1 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h1><h2 id="Different-Methods-for-Calculating-the-Differential"><a href="#Different-Methods-for-Calculating-the-Differential" class="headerlink" title="Different Methods for Calculating the Differential"></a>Different Methods for Calculating the Differential</h2><ul><li>Numerical: limit of slope</li><li>Symbolic: similar to Wolfram, returns symbolic expression of derivative</li><li><strong>Automatic</strong>: uses the chain rule. For each step, the upstream derivative is passed in. Then the respective gradient-calculating function for the forward function has the mathematical formula for the single-step derivative, into which numerical values are plugged in. It outputs an accurate downstream numerical derivative</li></ul><h2 id="Variables’-Fields"><a href="#Variables’-Fields" class="headerlink" title="Variables’ Fields"></a>Variables’ Fields</h2><ul><li><code>data</code>: tensor</li><li><code>grad</code>: the numerical gradient of the variable, calculated after a backward pass</li><li><code>grad_fn</code>: which gradient-calculating function should the variable use, determined by the previous operation in the forward tree</li><li><code>is_leaf</code></li><li><code>requires_grad</code></li><li><code>_version</code>: used to track whether the variable’s data has changed after it was saved by <code>ctx</code></li></ul><h2 id="The-Backward-Tree"><a href="#The-Backward-Tree" class="headerlink" title="The Backward Tree"></a>The Backward Tree</h2><p>A backward tree is constructed according to the forward tree.</p><ul><li>For each variable with a <code>grad_fn</code>, a gradient-calculating function node is added to the tree</li><li>For each operation needing inputs to calculate the gradient (e.g. multiplication), the operation saves the input variables and add them to the corresponding gradient-calculating function node through a context variable <code>ctx</code></li><li>For each variable with <code>is_leaf == True</code> and <code>required_grad == True</code>, <code>accumulate_grad</code> is called, which takes gradients from the previous gradient-calculating function node, aggregates it, and saves it to the variable’s <code>grad</code> field</li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li><a target="_blank" rel="noopener" href="http://blog.ezyang.com/2019/05/pytorch-internals/">Pytorch Internals - ezyang’s blog</a></li><li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=MswxJw-8PvE">PyTorch Autograd Explained</a></li><li><a target="_blank" rel="noopener" href="https://medium.com/analytics-vidhya/pytorch-contiguous-vs-non-contiguous-tensor-view-understanding-view-reshape-73e10cdfa0dd">Contiguous vs Non-Contiguous Tensor &#x2F; View — Understanding view(), reshape(), transpose()</a></li></ol><p></p></div><div class="share"><span>分享到</span>&nbsp;<span class="soc"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></span><span class="soc"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></span><span class="soc"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=https://termina.me/pytorch-internals/%20termina's Blog%20Pytorch Internals: Tensors and Autograd"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/second-year/" title="本科两年，得到的与失去的"><i class="fa fa-angle-double-left"></i>&nbsp;上一篇: 本科两年，得到的与失去的</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/20th-birthday/" title="[旧文]20岁">下一篇: [旧文]20岁&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2023&nbsp;<a target="_blank" href="https://termina.me" rel="noopener noreferrer">termina</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>